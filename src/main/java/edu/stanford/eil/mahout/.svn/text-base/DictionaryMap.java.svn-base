/**
 * 
 */
package edu.stanford.eil.mahout;

import java.io.BufferedReader;
import java.io.FileReader;
import java.util.HashMap;
import java.util.Map;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.mahout.math.NamedVector;
import org.apache.mahout.math.RandomAccessSparseVector;
import org.apache.mahout.math.Vector.Element;
import org.apache.mahout.math.VectorWritable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import edu.stanford.eil.store.FedStore;


/**
 * Letâ€™s say that for any given (unique) document id (from within a document corpus), 
 * we want to see what are the tokens that received the biggest weights 
 * in order to feel what are the most significant tokens of that 
 * document (as the weighting scheme sees it).
 * To do so, we can for instance load the content of a generated 
 * dictionary file (generated by Mahout) into a Map with token 
 * index as keys and the tokens as values. In this instance we call 
 * the map dictionaryMap, the class name shadows this convention.
 * We then have to walk through the generated binary file (A Hadoop sequence file
 * also generated by Mahout and residing on HDFS) containing the vectors. 
 * @author lewismc
 *
 */
public class DictionaryMap {

  /** The logging implementation */
  private static final Logger log = LoggerFactory.getLogger(DictionaryMap.class);
  
  /**
   * default constructor
   */
  public DictionaryMap() {
  }
  
  public void createDictionaryMap(String vectorPath, String dictionary) {
    Configuration conf = new Configuration();
    //FileSystem fs = FileSystem.get(conf);
    //String vectorsPath = vectorPath;
    //Path path = new Path(vectorsPath);
   
    //read in keys (terms) and values (doc frequency) from the dictionary
    //store them in a HashMap.
    //HashMap<String, String> dictionaryMap = new HashMap<String, String>();
    //log.info("Parsing file:" + dictionary);
    //BufferedReader br = new BufferedReader(new FileReader(dictionary));
    //int lineCount = 0;
    //try {
      //String line = br.readLine();
      //do {
        //StringTokenizer matcher = new StringTokenizer(line);
        //String term = matcher.nextToken();
        //String docFreq = matcher.nextToken();
        //dictionaryMap.put(term, docFreq);
        //log.debug("Added term: " + term + " with doc frequency value: " + docFreq + " to dictionaryMap.");
        //lineCount++;
        //line = br.readLine();
      //} 
      //while(line != null);
    //} finally {
      //br.close();  
   // }
    //log.info("Finished parsing file: " + dictionary + ". Added " + lineCount + " terms to map");
    
    //SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);
    //LongWritable key = new LongWritable();
    //VectorWritable value = new VectorWritable();
    
    //while (reader.next(key, value)) {
    //  NamedVector namedVector = (NamedVector)value.get();
    //  RandomAccessSparseVector vect = (RandomAccessSparseVector)namedVector.getDelegate();
   
    //  for( Element  e : vect ){
    //    log.info("Token: "+ dictionaryMap.get(e.index())+", TF-IDF weight: "+e.get()) ;
    //  }
    //}
    //reader.close();
  }

  
  
  /**
   * so this class can be invoked from the command line
   * @param args
   */
  public static void main(String[] args) {
    

  }

}
